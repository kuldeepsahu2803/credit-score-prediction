# -*- coding: utf-8 -*-
"""Credit Score Model .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Eehfqy3e6H_bnnX9OBsXWUBk5U2xnLc
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load the datasets
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Data Preprocessing (Minimal Version)
# Drop irrelevant columns. Check if they exist in the DataFrame first
columns_to_drop = ['ID', 'Customer_ID', 'Name', 'SSN', 'Month']

# Check for existence and drop only existing columns
existing_columns = [col for col in columns_to_drop if col in train_df.columns]
train_df = train_df.drop(columns=existing_columns)

# Select only numeric columns for median imputation
numeric_cols = train_df.select_dtypes(include=['number']).columns

# Fill missing values in numeric columns with the median
train_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].median())

# Encode categorical features
le = LabelEncoder()
for col in train_df.select_dtypes(include=['object']).columns:
    train_df[col] = le.fit_transform(train_df[col].astype(str))

# Splitting dataset into features (X) and target (y)
X = train_df.drop(columns=['Credit_Score'])  # Replace 'Credit_Score' with the actual target column
y = train_df['Credit_Score']

# Split the training data into train and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Training
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Model Prediction
y_pred = model.predict(X_val)

# Model Evaluation
accuracy = accuracy_score(y_val, y_pred)
print(f"Validation Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", classification_report(y_val, y_pred))

# Data Preprocessing (Minimal Version)
# Drop irrelevant columns. Check if they exist in the DataFrame first
columns_to_drop = ['ID', 'Customer_ID', 'Name', 'SSN', 'Month']

# Check for existence and drop only existing columns
existing_columns = [col for col in columns_to_drop if col in train_df.columns]
train_df = train_df.drop(columns=existing_columns)

# Select only numeric columns for median imputation
numeric_cols = train_df.select_dtypes(include=['number']).columns

# Fill missing values in numeric columns with the median
train_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].median())


# Prepare the test data
# Check for existence and drop only existing columns in test_df
existing_columns = [col for col in columns_to_drop if col in test_df.columns]
test_df = test_df.drop(columns=existing_columns)

# Select only numeric columns for median imputation in test_df
numeric_cols_test = test_df.select_dtypes(include=['number']).columns

# Fill missing values in numeric columns with the median
test_df[numeric_cols_test] = test_df[numeric_cols_test].fillna(test_df[numeric_cols_test].median())


# Encode categorical features
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

# Fit LabelEncoder on all unique values from both train and test
for col in train_df.select_dtypes(include=['object']).columns:
    # Combine unique values from both dataframes
    all_values = pd.concat([train_df[col], test_df[col]]).unique()
    # Fit the encoder on all unique values
    le.fit(all_values.astype(str))
    # Transform both train and test data
    train_df[col] = le.transform(train_df[col].astype(str))
    test_df[col] = le.transform(test_df[col].astype(str))

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the datasets
train_df = pd.read_csv('train.csv')

# Distribution of Numerical Features
def plot_numerical_distributions(df):
    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns
    df[numerical_cols].hist(bins=15, figsize=(15, 10), layout=(4, 4), color='skyblue', edgecolor='black')
    plt.suptitle('Distribution of Numerical Features', fontsize=16)
    plt.tight_layout()
    plt.show()

# Call the functions for visualization
plot_numerical_distributions(train_df)

def get_column_details(df, column_name):
  print(f"Column: {column_name}")
  print(f"Data Type: {df[column_name].dtype}")
  print(f"Number of Unique Values: {df[column_name].nunique()}")
  print(f"Missing Values: {df[column_name].isnull().sum()}")
  print(f"Value Counts:\n{df[column_name].value_counts()}")

def plot_countplot(df, column_name, user_friendly_name):
  plt.figure(figsize=(8, 5))
  sns.countplot(data=df, x=column_name)
  plt.title(f'Distribution of {user_friendly_name}', fontsize=14)
  plt.xlabel(user_friendly_name, fontsize=12)
  plt.ylabel('Count', fontsize=12)
  plt.xticks(rotation=45, ha='right')
  plt.tight_layout()
  plt.show()

  def plot_stacked_bar(df, column_name, hue_column):
    plt.figure(figsize=(8, 5))
   # Use crosstab to get counts for each combination
    ct = pd.crosstab(df[column_name], df[hue_column])
  # Plot the stacked bar chart
    ct.plot(kind='bar', stacked=True)
    plt.title(f'Distribution of {hue_column} by {column_name}', fontsize=14)
    plt.xlabel(column_name, fontsize=12)
    plt.ylabel('Count', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.legend(title=hue_column)
    plt.tight_layout()
    plt.show()

column_name = 'Credit_Score'
user_friendly_name = 'Credit Score'
# Get Details
get_column_details(df_train, column_name)
# Plot Graph
plot_countplot(df_train, column_name, user_friendly_name)

column_name = 'Month'
#Get Details
get_column_details(df_train,column_name)
#Plot Distrbution with Credit_Score
plot_stacked_bar(df_train,column_name,'Credit_Score')

# 1. Bar Plot: Average Credit Score per Month
def plot_avg_credit_score_per_month(df):
    plt.figure(figsize=(10, 6))
    sns.barplot(data=df, x='Month', y='Credit_Score', palette='Blues_d', ci=None)
    plt.title('Average Credit Score per Month', fontsize=16)
    plt.xlabel('Month')
    plt.ylabel('Average Credit Score')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# Call the functions to generate the visualizations
plot_avg_credit_score_per_month(train_df)

